{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnndissect.utils.model_utils import predict_sentiment\n",
    "import rnndissect.utils.nlp_utils as nlpu\n",
    "import rnndissect.activations.extractor as extr\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "sys.path.append(\"../model\")\n",
    "from lstm import LSTM\n",
    "from configs import *\n",
    "\n",
    "with open(\"/home/goncalo/Documents/rnndissect/assets/imdb_vocab.pkl\", \"rb\") as vf:\n",
    "    vocab = pickle.load(vf)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The movie ends an era of the franchise by taking a second stab at adapting a classic comics arc, with deeply disappointing results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and import the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=7635)\n",
       "  (rnn): LSTM(100, 256, num_layers=2)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0\n",
    "PAD_IDX = vocab.stoi['pad']\n",
    "\n",
    "config = LSTM_CONFIG1\n",
    "config.output_dim = 2\n",
    "model = LSTM(25002, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "model.load_state_dict(torch.load(\"../model/lstmo.pt\"))\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    logit = model(tensor, length_tensor)\n",
    "    prediction = torch.sigmoid(logit)\n",
    "    print(logit)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a random initial input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape should be n_words x embedding_dim\n",
    "# how many n_words to optimize the output? try different lengths\n",
    "\n",
    "min_emb, max_emb = model.embedding.weight.min().item(), model.embedding.weight.max().item()\n",
    "\n",
    "def generate_random_input(shape, min_lim, max_lim):\n",
    "    \"\"\"shape is a pair tuple\n",
    "       uniform distribution is used\"\"\"\n",
    "    return torch.FloatTensor(shape[0], shape[1]).uniform_(min_lim, max_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation maximization begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def am_loop(lr, x, model, thresh):\n",
    "    diff = float(\"inf\")\n",
    "    iters = 0\n",
    "    model.train()\n",
    "    \n",
    "    lstm_out, _ = model.rnn(x.view(1, 1, -1))\n",
    "    prev_logits = model.fc(lstm_out)\n",
    "    prev_logits.backward()\n",
    "    \n",
    "    while diff > thresh:\n",
    "        iters += 1\n",
    "        x.requires_grad = False\n",
    "        x = x + lr * x.grad\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        lstm_out, _ = model.rnn(x.view(1, 1, -1))\n",
    "        logits = model.fc(lstm_out)\n",
    "        logits.backward()\n",
    "        diff = (logits - prev_logits).sum().item()\n",
    "        prev_logits = logits\n",
    "    print(iters)\n",
    "    print(logits)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_random_input((1, 100), min_emb, max_emb).to(DEVICE)\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137824\n",
      "tensor([[[4.0134]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "174798\n",
      "tensor([[[4.1065]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "124806\n",
      "tensor([[[4.1561]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "179007\n",
      "tensor([[[4.0760]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "141605\n",
      "tensor([[[4.2206]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "\n",
    "for _ in range(5):\n",
    "    x = generate_random_input((1, 100), min_emb, max_emb).to(DEVICE)\n",
    "    x.requires_grad = True\n",
    "    res_list.append(am_loop(0.01, x, model, 1e-6)[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4575]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38757142424583435"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"Wyatt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_max(lst):\n",
    "    return lst.index(min(lst))\n",
    "\n",
    "def get_n_most_similar(vec, vecs, vocab, n=10):\n",
    "    words = []\n",
    "    sims = []\n",
    "    best_idx = 0\n",
    "    best_sim = 0\n",
    "    worst_sim_idx = 0\n",
    "    for i in range(len(vecs)):\n",
    "        curr_sim = cosine_similarity([vec], [vecs[i].numpy()])\n",
    "        if len(words) == 10:\n",
    "            if curr_sim > sims[worst_sim_idx]:\n",
    "                words[worst_sim_idx] = i\n",
    "                sims[worst_sim_idx] = curr_sim\n",
    "                worst_sim_idx = index_of_max(sims)\n",
    "        else:\n",
    "            words.append(i)\n",
    "            sims.append(curr_sim)\n",
    "    return list(vocab.itos[i] for i in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['riddled',\n",
       " 'grenade',\n",
       " 'wound',\n",
       " 'bullets',\n",
       " 'chest',\n",
       " 'stab',\n",
       " 'shotgun',\n",
       " 'bullet',\n",
       " 'gunshot',\n",
       " 'wounds']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_most_similar(vocab.vectors[vocab.stoi[\"bullet\"]].numpy(), vocab.vectors, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
